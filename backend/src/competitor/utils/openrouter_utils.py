"""
OpenRouter LLM utilities - ä¸ç°æœ‰LLMæ¥å£å®Œå…¨å…¼å®¹çš„OpenRouterå®ç°

è¿™ä¸ªæ–‡ä»¶æä¾›äº†ä¸€ä¸ªæ›¿ä»£çš„LLMå®ç°ï¼Œå¯ä»¥æ— ç¼æ›¿æ¢åŸæœ‰çš„Claudeè°ƒç”¨ã€‚
ä½¿ç”¨æ–¹æ³•ï¼šè®¾ç½®ç¯å¢ƒå˜é‡ USE_OPENROUTER=true å³å¯è‡ªåŠ¨åˆ‡æ¢ã€‚

æ”¯æŒçš„æ¨¡å‹ï¼š
- anthropic/claude-3.5-sonnet
- openai/gpt-4-turbo
- openai/gpt-4o
- meta-llama/llama-3.1-405b
ç­‰ç­‰...
"""

import os
import asyncio
import aiohttp
import json
from typing import Dict, Any, Optional
from .rate_limiter import RateLimiter


class OpenRouterManager:
    """OpenRouter LLMç®¡ç†å™¨ï¼Œä¸ç°æœ‰LLMManageræ¥å£å®Œå…¨å…¼å®¹"""
    
    def __init__(self, 
                 model_name: str = "anthropic/claude-3.5-sonnet",
                 temperature: float = 0.15,
                 max_tokens: int = 4096,
                 rate_limiter: Optional[RateLimiter] = None):
        """
        åˆå§‹åŒ–OpenRouterç®¡ç†å™¨
        
        Args:
            model_name: æ¨¡å‹åç§° (OpenRouteræ ¼å¼ï¼Œå¦‚ anthropic/claude-3.5-sonnet)
            temperature: æ¸©åº¦è®¾ç½®
            max_tokens: æœ€å¤§tokenæ•°
            rate_limiter: å¯é€‰çš„é€Ÿç‡é™åˆ¶å™¨
        """
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.rate_limiter = rate_limiter or RateLimiter(model_max_tokens=max_tokens)
        
        # OpenRouter APIé…ç½®
        self.api_key = os.getenv("OPENROUTER_API_KEY")
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
        
        self.base_url = "https://openrouter.ai/api/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "HTTP-Referer": "https://github.com/leviton-switch-demo",  # å¯é€‰ï¼šç”¨äºanalytics
            "X-Title": "Leviton Switch Competitor Analysis",  # å¯é€‰ï¼šç”¨äºanalytics
            "Content-Type": "application/json"
        }
    
    async def safe_call(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """
        å®‰å…¨çš„å¼‚æ­¥LLMè°ƒç”¨ï¼Œä¸ç°æœ‰æ¥å£å®Œå…¨å…¼å®¹
        
        Args:
            prompt: å‘é€ç»™LLMçš„æç¤º
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        estimated_input_tokens = self.rate_limiter.estimate_tokens(prompt)
        
        try:
            # è·å–é€Ÿç‡é™åˆ¶è®¸å¯
            await self.rate_limiter.acquire(estimated_input_tokens)
            
            # å‡†å¤‡OpenRouter APIè¯·æ±‚
            payload = {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "stream": False
            }
            
            # å‘é€å¼‚æ­¥è¯·æ±‚
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.base_url,
                    headers=self.headers,
                    json=payload,
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise Exception(f"OpenRouter APIé”™è¯¯ {response.status}: {error_text}")
                    
                    result = await response.json()
            
            # æå–å“åº”å†…å®¹
            response_text = result["choices"][0]["message"]["content"].strip()
            
            # æ›´æ–°é€Ÿç‡é™åˆ¶å™¨çš„å®é™…ä½¿ç”¨é‡
            actual_input_tokens = result.get("usage", {}).get("prompt_tokens", estimated_input_tokens)
            actual_output_tokens = result.get("usage", {}).get("completion_tokens", len(response_text) // 4)
            
            self.rate_limiter.release(actual_input_tokens, actual_output_tokens)
            
            return response_text
            
        except Exception as e:
            self.rate_limiter.release()
            print(f"âŒ OpenRouter LLMè°ƒç”¨å¤±è´¥: {e}")
            raise


class OpenRouterResponse:
    """æ¨¡æ‹ŸLangChainå“åº”å¯¹è±¡ï¼Œç¡®ä¿å…¼å®¹æ€§"""
    
    def __init__(self, content: str, usage_metadata: Dict[str, int] = None):
        self.content = content
        self.usage_metadata = usage_metadata or {}


class OpenRouterLLM:
    """æ¨¡æ‹ŸLangChain ChatAnthropicæ¥å£çš„OpenRouterå®ç°"""
    
    def __init__(self, model: str, temperature: float, max_tokens: int):
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.manager = OpenRouterManager(model, temperature, max_tokens)
    
    async def ainvoke(self, prompt: str) -> OpenRouterResponse:
        """å¼‚æ­¥è°ƒç”¨ï¼Œæ¨¡æ‹ŸLangChainçš„ainvokeæ¥å£"""
        try:
            # å¦‚æœä¼ å…¥çš„æ˜¯stringï¼Œç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œæå–content
            if hasattr(prompt, 'content'):
                prompt_text = prompt.content
            elif isinstance(prompt, str):
                prompt_text = prompt
            else:
                prompt_text = str(prompt)
            
            response_text = await self.manager.safe_call(prompt_text)
            
            # åˆ›å»ºå…¼å®¹çš„å“åº”å¯¹è±¡
            return OpenRouterResponse(
                content=response_text,
                usage_metadata={
                    'input_tokens': self.manager.rate_limiter.estimate_tokens(prompt_text),
                    'output_tokens': self.manager.rate_limiter.estimate_tokens(response_text)
                }
            )
            
        except Exception as e:
            print(f"âŒ OpenRouter ainvokeå¤±è´¥: {e}")
            raise


# === æ™ºèƒ½åˆ‡æ¢æœºåˆ¶ ===

def should_use_openrouter() -> bool:
    """æ£€æŸ¥æ˜¯å¦åº”è¯¥ä½¿ç”¨OpenRouter"""
    return os.getenv("USE_OPENROUTER", "false").lower() in ("true", "1", "yes")


def get_openrouter_model_mapping(claude_model: str) -> str:
    """å°†Claudeæ¨¡å‹åç§°æ˜ å°„åˆ°OpenRouteræ¨¡å‹åç§°"""
    mapping = {
        "claude-sonnet-4-20250514": "anthropic/claude-3.5-sonnet",
        "claude-3-sonnet": "anthropic/claude-3-sonnet",
        "claude-3-opus": "anthropic/claude-3-opus",
        "claude-3-haiku": "anthropic/claude-3-haiku",
    }
    return mapping.get(claude_model, "anthropic/claude-3.5-sonnet")


# === å…¼å®¹æ€§åŒ…è£…å™¨ ===

class CompatibleLLMManager:
    """å…¼å®¹çš„LLMç®¡ç†å™¨ï¼Œè‡ªåŠ¨é€‰æ‹©OpenRouteræˆ–åŸå§‹Claude"""
    
    def __init__(self, 
                 model_name: str = "claude-sonnet-4-20250514",
                 temperature: float = 0.15,
                 max_tokens: int = 4096,
                 rate_limiter: Optional[RateLimiter] = None):
        
        if should_use_openrouter():
            print("ğŸ”€ ä½¿ç”¨OpenRouterä½œä¸ºLLMæä¾›å•†")
            openrouter_model = get_openrouter_model_mapping(model_name)
            self.manager = OpenRouterManager(openrouter_model, temperature, max_tokens, rate_limiter)
            self.is_openrouter = True
        else:
            print("ğŸ”µ ä½¿ç”¨åŸå§‹Claude API")
            # å¯¼å…¥åŸå§‹çš„LLMManager
            from .llm_utils import LLMManager
            self.manager = LLMManager(model_name, temperature, max_tokens, rate_limiter)
            self.is_openrouter = False
    
    async def safe_call(self, prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
        """ç»Ÿä¸€çš„å®‰å…¨è°ƒç”¨æ¥å£"""
        return await self.manager.safe_call(prompt, context)
    
    @property
    def llm(self):
        """è·å–åº•å±‚LLMå¯¹è±¡"""
        if self.is_openrouter:
            return OpenRouterLLM(self.manager.model_name, self.manager.temperature, self.manager.max_tokens)
        else:
            return self.manager.llm


# === å…¨å±€ç®¡ç†å™¨æ›¿æ¢å‡½æ•° ===

def create_compatible_llm_manager(model_name: str = "claude-sonnet-4-20250514",
                                temperature: float = 0.15,
                                max_tokens: int = 4096,
                                rate_limiter: Optional[RateLimiter] = None):
    """åˆ›å»ºå…¼å®¹çš„LLMç®¡ç†å™¨"""
    return CompatibleLLMManager(model_name, temperature, max_tokens, rate_limiter)


# === ä¾¿æ·çš„åˆ‡æ¢å‡½æ•° ===

async def safe_llm_call_openrouter(prompt: str, context: Optional[Dict[str, Any]] = None) -> str:
    """
    ä½¿ç”¨OpenRouterçš„å®‰å…¨LLMè°ƒç”¨
    è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥æ›¿æ¢åŸæœ‰çš„safe_llm_call
    """
    manager = CompatibleLLMManager()
    return await manager.safe_call(prompt, context)


# === æ¨¡å‹é€‰æ‹©æŒ‡å— ===

def get_recommended_models() -> Dict[str, str]:
    """è·å–æ¨èçš„OpenRouteræ¨¡å‹"""
    return {
        "claude-3.5-sonnet": "anthropic/claude-3.5-sonnet",  # æ¨èï¼šå¹³è¡¡æ€§èƒ½å’Œæˆæœ¬
        "claude-3-opus": "anthropic/claude-3-opus",  # æœ€é«˜è´¨é‡ï¼Œè¾ƒè´µ
        "gpt-4-turbo": "openai/gpt-4-turbo",  # OpenAIæœ€æ–°æ¨¡å‹
        "gpt-4o": "openai/gpt-4o",  # å¤šæ¨¡æ€æ”¯æŒ
        "llama-3.1-405b": "meta-llama/llama-3.1-405b",  # å¼€æºé«˜æ€§èƒ½
        "gemini-pro": "google/gemini-pro",  # Googleæ¨¡å‹
    }


def print_setup_instructions():
    """æ‰“å°è®¾ç½®è¯´æ˜"""
    print("""
ğŸš€ OpenRouterè®¾ç½®è¯´æ˜ï¼š

1. è·å–API Keyï¼š
   - è®¿é—® https://openrouter.ai/
   - æ³¨å†Œè´¦æˆ·å¹¶è·å–APIå¯†é’¥

2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
   export OPENROUTER_API_KEY="your-api-key"
   export USE_OPENROUTER=true

3. é€‰æ‹©æ¨¡å‹ï¼ˆå¯é€‰ï¼‰ï¼š
   export OPENROUTER_MODEL="anthropic/claude-3.5-sonnet"

4. è¿è¡Œè„šæœ¬ï¼š
   python3 final_assign_aspects_only.py

ğŸ’¡ ä¼˜åŠ¿ï¼š
   - æ”¯æŒå¤šä¸ªAIæä¾›å•†
   - ç»Ÿä¸€çš„APIæ¥å£
   - é€šå¸¸æ›´ä¾¿å®œ
   - æ— åœ°åŸŸé™åˆ¶
   - æ”¯æŒæ›´å¤šæ¨¡å‹é€‰æ‹©
""")


if __name__ == "__main__":
    print_setup_instructions() 